---
title:  Day 1
layout: singletrack
tagline: First day of the workshop, covering GP approximations, differential equations and score estimation.
show_abstracts: true
room:
talks:
- title: "Arrivals"
  start: "8:30"
  end: "8:45"
- title: "Welcome"
  speaker: "Ralf Herbrich"
  url: "http://herbrich.me/wp/"
  start: "8:45"
  end: "9:00"
  institute: "Amazon"
- start: "9:00"
  end: "9:30"
  title: A Unifying Framework for Sparse Gaussian Process Approximation using Power Expectation Propagation
  abstract: "The application of GPs is limited by computational and analytical intractabilities that arise when data are sufficiently numerous or when employing non-Gaussian models. A wealth of GP approximation schemes have been developed over the last 15 years to address these key limitations. Many of these schemes employ a small set of pseudo data points to summarise the actual data. We have developed a new pseudo-point approximation framework using Power Expectation Propagation (Power EP) that unifies a large number of these pseudo-point approximations. The new framework is built on standard methods for approximate inference (variational free-energy, EP and power EP methods) rather than employing approximations to the probabilistic generative model itself. In this way all of approximation is performed at `inference time' rather than at `modelling time' resolving awkward philosophical and empirical questions that trouble previous approaches. Crucially, we demonstrate that the new framework includes new pseudo-point approximation methods that outperform current approaches on regression, classification and state space modelling tasks in batch and online settings."
  speaker: Richard Turner
  url: http://cbl.eng.cam.ac.uk/Public/Turner/WebHome
  institute: University of Cambridge
  keywords: Gaussian processes, power expectation propagation, online inference
  co-authors: Thang D. Bui, Cuong Viet Nguyen
- start: '9:45'
  end: '10:15'
  title: Scalable Multi-Class Gaussian Process Classification using Expectation Propagation
  abstract: We describe an expectation propagation (EP) method for multi-class classification with Gaussian processes that scales well to very large datasets. In such a method the estimate of the log-marginal-likelihood involves a sum across the data instances. This enables efficient training using stochastic gradients and mini-batches. When this type of training is used, the computational cost does not depend on the number of data instances N. Furthermore, extra assumptions in the approximate inference process make the memory cost independent of N. The consequence is that the proposed EP method can be used on datasets with millions of instances. We compare empirically this method with alternative approaches that approximate the required computations using variational inference. The results show that it performs similar or even better than these techniques, which sometimes give significantly worse predictive distributions in terms of the test log-likelihood. Besides this, the training process of the proposed approach also seems to converge in a smaller number of iterations.
  speaker: Daniel Hernández-Lobato
  url: https://dhnzl.org/
  institute: Universidad Autónoma de Madrid
  keywords: Approximate Inference, Expectation Propagation, Multi-class classification
  co-authors: Carlos Villacampa-Calvo (first author)
- start: '10:30'
  end: '11:00'
  title: "Coffee"
- start: '11:00'
  end: '11:30'
  title: Doubly Stochastic Variational Inference for Deep Gaussian Processes
  abstract: Deep Gaussian Processes (DGPs) provide a Bayesian non-parametric alternative to traditional deep networks. A variational objective can be derived in closed form if the variational posterior is forced to factorize between and within layers, but this severe independence assumption does not work well in practice and does not readily scale to large data. We present a doubly stochastic variational inference algorithm that does not force independence between layers. The first source of stochasticity is Monte Carlo sampling of the lower bound. This allows us to use a rich posterior that matches the structure of the model. The second source of stochasticity is minibatch sub-sampling, permitting inference on very large data. With our approach we show that DGPs outperform shallow models on a wide range of benchmark classification and regression tasks, ranging in size from hundreds of data to tens of millions.  
  speaker: Hugh Salimbeni
  url: http://wp.doc.ic.ac.uk/sml/person/hugh-salimbeni/
  institute: Imperial College London
  keywords: stochastic variational inference, deep Gaussian processes, big data
  co-authors: Marc Deisenroth
- start: '11:45'
  end: '12:15'
  title: Stochastic (Partial) Differential Equation Methods for Gaussian Processes
  abstract: Stochastic partial differential equations and stochastic differential equations can be seen as alternatives to kernels in representation of Gaussian processes in machine learning and inverse problems. Linear operator equations correspond to spatial kernels, and temporal kernels are equivalent to linear Ito stochastic differential equations. The differential equation representations allow for the use of differential equation numerical methods on Gaussian processes. For example, finite-differences, finite elements, basis function methods, and Galerkin methods can be used. In temporal and spatio-temporal case we can use linear-time Kalman filter and smoother approaches.
  speaker: Simo Särkkä
  url: https://users.aalto.fi/~ssarkka/
  institute: Aalto University
  keywords: Stochastic partial differential equation, stochastic calculus, random field
  co-authors:
- start: '12:30'
  end: '13:30'
  title: "Lunch"
- start: '13:30'
  end: '14:00'
  title: Variational Fourier Features for Gaussian Processes
  abstract: This work brings together two powerful concepts in Gaussian processes':' the variational approach to sparse approximation and the spectral representation of Gaussian processes. This gives rise to an approximation that inherits the benefits of the variational approach but with the representational power and computational scalability of spectral representations. The work hinges on a key result that there exist spectral features related to a finite domain of the Gaussian process which exhibit almost-independent covariances. We derive these expressions for Matern kernels in one dimension, and generalize to more dimensions using kernels with specific structures. Under the assumption of additive Gaussian noise, our method requires only a single pass through the dataset, making for very fast and accurate computation. We fit a model to 4 million training points in just a few minutes on a standard laptop. With non-conjugate likelihoods, our MCMC scheme reduces the cost of computation from O(NM2) (for a sparse Gaussian process) to O(NM) per iteration, where N is the number of data and M is the number of features.
  speaker: James Hensman
  url: http://jameshensman.github.io/
  institute: prowler.io
  keywords: Fourier methods; Variational approximations
  co-authors: Nicolas Durrande, Arno Solin
- start: '14:15'
  end: '14:45'
  title: Projection predictive model reduction for Gaussian process models
  abstract: We propose a new method for simplification of Gaussian process (GP) models by projecting the information contained in the full encompassing model and selecting a reduced number of variables based on their predictive relevance.  Our results on synthetic and real world datasets show that the proposed method improves the assessment of variable relevance compared to the automatic relevance determination (ARD) via the length-scale parameters.  We expect the method to be useful for improving explainability of the models, reducing the future measurement costs and reducing the computation time for making new predictions.
  speaker: Aki Vehtari
  url: https://users.aalto.fi/~ave/
  institute: Aalto University
  keywords: Gaussian processes, model selection, Bayesian inference
  co-authors: Juho Piironen
- start: '15:00'
  end: '15:30'
  title: "Tea"
- start: '15:30'
  end: '16:00'
  title: Efficient and principled score estimation
  abstract: We introduce and analyse an algorithm to efficiently estimate a dataset's score function, i.e the derivative of the log-density. Our work builds on a recently proposed score-matching estimator for an infinite dimensional exponential family model in a reproducing kernel Hilbert space. To overcome the estimator's prohibitive computational costs, cubic in both dimensions D and samples N, we apply the Nyström method':' by representing the solution to the estimation problem using only a sub-sample of all data, we significantly reduce run-time and memory usage. We present initial and promising work towards both consistency of the approximate estimator and generalisation error analysis for the random design setting, using ideas from recent theoretical breakthroughs for Nyström kernel least-squares. We compare our method to the popular de-noising autoencoder and previous approximations of the kernel model. In addition to the lack of theoretical analysis of the auto-encoder methods, an empirical comparison shows that our estimator performs favourably':' it is more data-efficient, has fewer parameters which can be tuned in a principled way, and behaves outside the range of the training data.
  speaker: Heiko Strathmann
  url: http://herrstrathmann.de/
  institute: Gatsby Unit, UCL
  keywords: kernels, low-rank approximations, learning bounds
  co-authors: Dougal Sutherland, Arthur Gretton
- start: '16:15'
  end: '16:45'
  title: TBD
  abstract: TBD
  speaker: Suchi Saria
  url: https://www.cs.jhu.edu/faculty/suchi-saria/
  institute: Johns Hopkins University
  keywords: TBD
  co-authors:
- start: '20:00'
  title: Workshop dinner
---
