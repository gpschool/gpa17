---
title:  Day 1
layout: singletrack
tagline:
show_abstracts: true
room:
talks:
- title: "Arrivals"
  start: "8:30"
  end: "8:45"
- title: "Welcome"
  speaker: "Ralf Herbrich"
  url: "http://herbrich.me/wp/"
  start: "8:45"
  end: "9:00"
  institute: "Amazon"
- start: "9:00"
  end: "9:30"
  title: Random matrix theory and free probability for Gaussian latent variable models
  abstract: Inference algorithms for Gaussian latent variable models such as Expectation Propagationand variational Gaussian approximations require frequent matrix inversions which make  applications to large systems nontrivial. On the other hand, there are approaches to approximate inference which seem to take advantage of the fact that the number of random variables in the model is large. An example are the AMP (Approximate Message Passing) types of algorithms which have been applied to compressed sensing. A crucial assumption is that certain large matrices in the problem can be considered as random. In this talk I will discuss basic results of the “free probability” approach to random matrix theory and its potential to simplify matrix operations for approximate inference.
  speaker: Manfred Opper
  url: https://www.ki.tu-berlin.de/menue/team/manfred_opper/forschungsgebiete/
  institute: TU Berlin
  keywords: Approximate inference, Gaussian latent variable models, random matrix theory
  co-authors: Burak Cakmak, Ole Winther
- start: '9:45'
  end: '10:15'
  title: Scalable Multi-Class Gaussian Process Classification using Expectation Propagation
  abstract: We describe an expectation propagation (EP) method for multi-class classification with Gaussian processes that scales well to very large datasets. In such a method the estimate of the log-marginal-likelihood involves a sum across the data instances. This enables efficient training using stochastic gradients and mini-batches. When this type of training is used, the computational cost does not depend on the number of data instances N. Furthermore, extra assumptions in the approximate inference process make the memory cost independent of N. The consequence is that the proposed EP method can be used on datasets with millions of instances. We compare empirically this method with alternative approaches that approximate the required computations using variational inference. The results show that it performs similar or even better than these techniques, which sometimes give significantly worse predictive distributions in terms of the test log-likelihood. Besides this, the training process of the proposed approach also seems to converge in a smaller number of iterations.
  speaker: Daniel Hernández-Lobato
  url: https://dhnzl.org/
  institute: Universidad Autónoma de Madrid
  keywords: Approximate Inference, Expectation Propagation, Multi-class classification
  co-authors: Carlos Villacampa-Calvo (first author)
- start: '10:30'
  end: '11:00'
  title: "Coffee"
- start: '11:00'
  end: '11:30'
  title: Doubly Stochastic Variational Inference for Deep Gaussian Processes
  abstract: Deep Gaussian Processes (DGPs) provide a Bayesian non-parametric alternative to traditional deep networks. A variational objective can be derived in closed form if the variational posterior is forced to factorize between and within layers, but this severe independence assumption does not work well in practice and does not readily scale to large data. We present a doubly stochastic variational inference algorithm that does not force independence between layers. The first source of stochasticity is Monte Carlo sampling of the lower bound. This allows us to use a rich posterior that matches the structure of the model. The second source of stochasticity is minibatch sub-sampling, permitting inference on very large data. With our approach we show that DGPs outperform shallow models on a wide range of benchmark classification and regression tasks, ranging in size from hundreds of data to tens of millions.  
  speaker: Hugh Salimbeni
  url: http://wp.doc.ic.ac.uk/sml/person/hugh-salimbeni/
  institute: Imperial College London
  keywords: stochastic variational inference, deep Gaussian processes, big data
  co-authors: Marc Deisenroth
- start: '11:45'
  end: '12:15'
  speaker: TBA
- start: '12:30'
  end: '13:30'
  title: "Lunch"
- start: '13:30'
  end: '14:00'
  title: Variational Fourier Features for Gaussian Processes
  abstract: This work brings together two powerful concepts in Gaussian processes':' the variational approach to sparse approximation and the spectral representation of Gaussian processes. This gives rise to an approximation that inherits the benefits of the variational approach but with the representational power and computational scalability of spectral representations. The work hinges on a key result that there exist spectral features related to a finite domain of the Gaussian process which exhibit almost-independent covariances. We derive these expressions for Matern kernels in one dimension, and generalize to more dimensions using kernels with specific structures. Under the assumption of additive Gaussian noise, our method requires only a single pass through the dataset, making for very fast and accurate computation. We fit a model to 4 million training points in just a few minutes on a standard laptop. With non-conjugate likelihoods, our MCMC scheme reduces the cost of computation from O(NM2) (for a sparse Gaussian process) to O(NM) per iteration, where N is the number of data and M is the number of features.
  speaker: James Hensman
  url: http://jameshensman.github.io/
  institute: prowler.io
  keywords: Fourier methods; Variational approximations
  co-authors: Nicolas Durrande, Arno Solin
- start: '14:15'
  end: '14:45'
  title: Projection predictive model reduction for Gaussian process models
  abstract: We propose a new method for simplification of Gaussian process (GP) models by projecting the information contained in the full encompassing model and selecting a reduced number of variables based on their predictive relevance.  Our results on synthetic and real world datasets show that the proposed method improves the assessment of variable relevance compared to the automatic relevance determination (ARD) via the length-scale parameters.  We expect the method to be useful for improving explainability of the models, reducing the future measurement costs and reducing the computation time for making new predictions.
  speaker: Aki Vehtari
  url: https://users.aalto.fi/~ave/
  institute: Aalto University
  keywords: Gaussian processes, model selection, Bayesian inference
  co-authors: Juho Piironen
- start: '15:00'
  end: '15:30'
  title: "Tea"
- start: '15:30'
  end: '16:00'
  title: TBA
  speaker: Cedric Archembeau
  url: http://www0.cs.ucl.ac.uk/staff/c.archambeau/
  institute: Amazon
  co-authors:
- start: '16:15'
  end: '16:45'
  title: TBA
  speaker: Zhenwen Dai
  url: http://zhenwendai.github.io/
  institute: Amazon
  co-authors:
- start: '20:00'
  title: Workshop dinner
---
