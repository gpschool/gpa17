---
title:  Day 2
layout: singletrack
tagline:
show_abstracts: true
room:
talks:
- title: "Arrivals"
  start: "8:30"
  end: "9:00"
- start: "9:00"
  end: "9:30"
  speaker: TBA
- start: '9:45'
  end: '10:15'
  title: A Unifying Framework for Sparse Gaussian Process Approximation using Power Expectation Propagation
  abstract: "The application of GPs is limited by computational and analytical intractabilities that arise when data are sufficiently numerous or when employing non-Gaussian models. A wealth of GP approximation schemes have been developed over the last 15 years to address these key limitations. Many of these schemes employ a small set of pseudo data points to summarise the actual data. We have developed a new pseudo-point approximation framework using Power Expectation Propagation (Power EP) that unifies a large number of these pseudo-point approximations. The new framework is built on standard methods for approximate inference (variational free-energy, EP and power EP methods) rather than employing approximations to the probabilistic generative model itself. In this way all of approximation is performed at `inference time' rather than at `modelling time' resolving awkward philosophical and empirical questions that trouble previous approaches. Crucially, we demonstrate that the new framework includes new pseudo-point approximation methods that outperform current approaches on regression, classification and state space modelling tasks in batch and online settings."
  speaker: Richard Turner
  url: http://cbl.eng.cam.ac.uk/Public/Turner/WebHome
  institute: University of Cambridge
  keywords: Gaussian processes, power expectation propagation, online inference
  co-authors: Thang D. Bui, Cuong Viet Nguyen
- start: '10:30'
  end: '11:00'
  title: "Coffee"
- start: '11:00'
  end: '11:30'
  title: TBA
- start: '11:45'
  end: '12:15'
  title: TBA
- start: '12:30'
  end: '13:30'
  title: "Lunch"
- start: '13:30'
  end: '14:00'
  title: Stochastic (Partial) Differential Equation Methods for Gaussian Processes
  abstract: Stochastic partial differential equations and stochastic differential equations can be seen as alternatives to kernels in representation of Gaussian processes in machine learning and inverse problems. Linear operator equations correspond to spatial kernels, and temporal kernels are equivalent to linear Ito stochastic differential equations. The differential equation representations allow for the use of differential equation numerical methods on Gaussian processes. For example, finite-differences, finite elements, basis function methods, and Galerkin methods can be used. In temporal and spatio-temporal case we can use linear-time Kalman filter and smoother approaches.
  speaker: Simo Särkkä
  url: https://users.aalto.fi/~ssarkka/
  institute: Aalto University
  keywords: Stochastic partial differential equation, stochastic calculus, random field
  co-authors:
- start: '14:15'
  end: '14:45'
  title: Efficient and principled score estimation
  abstract: We introduce and analyse an algorithm to efficiently estimate a dataset's score function, i.e the derivative of the log-density. Our work builds on a recently proposed score-matching estimator for an infinite dimensional exponential family model in a reproducing kernel Hilbert space. To overcome the estimator's prohibitive computational costs, cubic in both dimensions D and samples N, we apply the Nyström method':' by representing the solution to the estimation problem using only a sub-sample of all data, we significantly reduce run-time and memory usage. We present initial and promising work towards both consistency of the approximate estimator and generalisation error analysis for the random design setting, using ideas from recent theoretical breakthroughs for Nyström kernel least-squares. We compare our method to the popular de-noising autoencoder and previous approximations of the kernel model. In addition to the lack of theoretical analysis of the auto-encoder methods, an empirical comparison shows that our estimator performs favourably':' it is more data-efficient, has fewer parameters which can be tuned in a principled way, and behaves outside the range of the training data.
  speaker: Heiko Strathmann
  url: http://herrstrathmann.de/
  institute: Gatsby Unit, UCL
  keywords: kernels, low-rank approximations, learning bounds
  co-authors: Dougal Sutherland, Arthur Gretton
- start: '15:00'
  end: '15:30'
  title: "Tea"
- start: '15:30'
  end: '16:00'
  title: TBA
  speaker: Suchi Saria
  url: https://www.cs.jhu.edu/faculty/suchi-saria/
  institute: Johns Hopkins University
  co-authors:
- start: '16:15'
  end: '16:45'
---
