---
title:  Day 2
layout: singletrack
tagline:
show_abstracts: true
room:
talks:
- title: "Arrivals"
  start: "8:30"
  end: "9:00"
- start: "9:00"
  end: "9:30"
  title: Adaptive Bayesian Quadrature for Approximate Inference -- a Case Study
  abstract: Despite its algebraic beauty, Bayesian quadrature (BQ) has found little practical use in machine learning. This is due to its cubic computational cost in the number of design points, and the fact that GP-based BQ does not scale to high-dimensional tasks. So what is necessary to make Bayesian integration a useable tool for approximate inference? I will report on an ongoing study that specifically targets GP binary classification (Bayesian logistic regression). In this concrete application, a nonlinearly-warped GP prior that gives rise to an adaptive BQ scheme, combined with careful algorithmic design, can make BQ practicable even for high-dimensional integration problems.
  speaker: Alexandra Gessner
  url: https://is.tuebingen.mpg.de/person/agessner
  institute: Max Planck Institute for Intelligent Systems
  keywords: Bayesian quadrature, approximate inference, Bayesian logistic regression
  co-authors: Philipp Hennig
- start: '9:45'
  end: '10:15'
  title: A Unifying Framework for Sparse Gaussian Process Approximation using Power Expectation Propagation
  abstract: "The application of GPs is limited by computational and analytical intractabilities that arise when data are sufficiently numerous or when employing non-Gaussian models. A wealth of GP approximation schemes have been developed over the last 15 years to address these key limitations. Many of these schemes employ a small set of pseudo data points to summarise the actual data. We have developed a new pseudo-point approximation framework using Power Expectation Propagation (Power EP) that unifies a large number of these pseudo-point approximations. The new framework is built on standard methods for approximate inference (variational free-energy, EP and power EP methods) rather than employing approximations to the probabilistic generative model itself. In this way all of approximation is performed at `inference time' rather than at `modelling time' resolving awkward philosophical and empirical questions that trouble previous approaches. Crucially, we demonstrate that the new framework includes new pseudo-point approximation methods that outperform current approaches on regression, classification and state space modelling tasks in batch and online settings."
  speaker: Richard Turner
  url: http://cbl.eng.cam.ac.uk/Public/Turner/WebHome
  institute: University of Cambridge
  keywords: Gaussian processes, power expectation propagation, online inference
  co-authors: Thang D. Bui, Cuong Viet Nguyen
- start: '10:30'
  end: '11:00'
  title: "Coffee"
- start: '11:00'
  end: '11:30'
  title: Advances in using GPs with derivative observations
  abstract: Derivative observations are a powerful and easy way to add information to GPs. However, they are not used as much as they could be. Our recent research has found several ways of how derivatives can be used to make GPs even more powerful, even if the derivatives can not be observed. In this talk I will discuss how derivative information can efficiently be used to detect monotonic functions, how it can help in finding minimum energy paths for physicists and how it can be used in Bayesian Optimization to reduce the number of acquisitions. Last we shortly discuss how the advances in EP methods allow us to scale the methods to even larger data sets.
  speaker: Eero Siivola
  url: https://users.aalto.fi/~siivole1/
  institute: Aalto University
- start: '11:45'
  end: '12:15'
  title: TBA
  speaker: Carl Henrik Ek
  url: http://www.carlhenrik.com/
  institute: University of Bristol
- start: '12:30'
  end: '13:30'
  title: "Lunch"
- start: '13:30'
  end: '14:00'
  title: Stochastic (Partial) Differential Equation Methods for Gaussian Processes
  abstract: Stochastic partial differential equations and stochastic differential equations can be seen as alternatives to kernels in representation of Gaussian processes in machine learning and inverse problems. Linear operator equations correspond to spatial kernels, and temporal kernels are equivalent to linear Ito stochastic differential equations. The differential equation representations allow for the use of differential equation numerical methods on Gaussian processes. For example, finite-differences, finite elements, basis function methods, and Galerkin methods can be used. In temporal and spatio-temporal case we can use linear-time Kalman filter and smoother approaches.
  speaker: Simo Särkkä
  url: https://users.aalto.fi/~ssarkka/
  institute: Aalto University
  keywords: Stochastic partial differential equation, stochastic calculus, random field
  co-authors:
- start: '14:15'
  end: '14:45'
  title: Efficient and principled score estimation
  abstract: We introduce and analyse an algorithm to efficiently estimate a dataset's score function, i.e the derivative of the log-density. Our work builds on a recently proposed score-matching estimator for an infinite dimensional exponential family model in a reproducing kernel Hilbert space. To overcome the estimator's prohibitive computational costs, cubic in both dimensions D and samples N, we apply the Nyström method':' by representing the solution to the estimation problem using only a sub-sample of all data, we significantly reduce run-time and memory usage. We present initial and promising work towards both consistency of the approximate estimator and generalisation error analysis for the random design setting, using ideas from recent theoretical breakthroughs for Nyström kernel least-squares. We compare our method to the popular de-noising autoencoder and previous approximations of the kernel model. In addition to the lack of theoretical analysis of the auto-encoder methods, an empirical comparison shows that our estimator performs favourably':' it is more data-efficient, has fewer parameters which can be tuned in a principled way, and behaves outside the range of the training data.
  speaker: Heiko Strathmann
  url: http://herrstrathmann.de/
  institute: Gatsby Unit, UCL
  keywords: kernels, low-rank approximations, learning bounds
  co-authors: Dougal Sutherland, Arthur Gretton
- start: '15:00'
  end: '15:30'
  title: "Tea"
- start: '15:30'
  end: '16:00'
  title: TBA
  speaker: Suchi Saria
  url: https://www.cs.jhu.edu/faculty/suchi-saria/
  institute: Johns Hopkins University
  co-authors:
- start: '16:15'
  end: '16:45'
  speaker: TBA
---
